{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86dd1ca-c568-44fd-ae70-2c06d8d69459",
   "metadata": {},
   "source": [
    "## 1) Encoder-only (BERT/DistilBERT) — sentence embedding + similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cfa7924-f39f-4cf4-8d68-9eebb0910e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim(sentence 0, 0) = 1.000 :: The cat sat on the mat.\n",
      "sim(sentence 0, 1) = 0.983 :: A dog rested on the rug.\n",
      "sim(sentence 0, 2) = 0.796 :: Transformers process sequences in parallel.\n"
     ]
    }
   ],
   "source": [
    "# Goal: Turn sentences into embeddings using an encoder-only model (DistilBERT)\n",
    "# and compute cosine similarity.\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "enc = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"A dog rested on the rug.\",\n",
    "    \"Transformers process sequences in parallel.\"\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch = tok(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    out = enc(**batch)                 # last hidden states (B, T, H)\n",
    "    cls_emb = out.last_hidden_state[:, 0, :]  # [CLS] token embedding as sentence vector (B, H)\n",
    "    cls_emb = F.normalize(cls_emb, p=2, dim=1)\n",
    "\n",
    "# cosine similarity between sentence 0 and others\n",
    "sims = (cls_emb @ cls_emb[0].unsqueeze(1)).squeeze(1)\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"sim(sentence 0, {i}) = {sims[i].item():.3f} :: {s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a26e1f-0caf-4eff-b904-e0d04c5d3fb6",
   "metadata": {},
   "source": [
    "## 2) Decoder-only (GPT-2) — text generation (causal LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e11674c-f2ef-4691-8522-0a89faf25031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee4e6326cff42419c13cae859a3ef43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0baa3ff87612445cb5261b3d9fc0c6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c6e9227c5d481c97ed09287ea9082c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7188d44ac3054e91baf5bdb998ab79b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51c336afb364c74a499e61044acefe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c18b0587e3c44cf849e2c5f15e45a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3417dbd8c034c7fb1c0af73b889cb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2017, researchers discovered that attention-seeking and attention-avoidance behaviors—such as having fewer kids—can cause children to have a higher risk of developing ADHD, and that children who are stressed out are at greater risk.\n",
      "\n",
      "\"Children's behaviors can also cause us to be more sensitive\n"
     ]
    }
   ],
   "source": [
    "# Goal: Generate text token-by-token with a decoder-only model (GPT-2).\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "lm  = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"In 2017, researchers discovered that attention\"\n",
    "inputs = tok(prompt, return_tensors=\"pt\")\n",
    "\n",
    "gen_ids = lm.generate(\n",
    "    **inputs,\n",
    "    max_length=60,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.8,\n",
    "    pad_token_id=tok.eos_token_id\n",
    ")\n",
    "print(tok.decode(gen_ids[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e6739-8fb0-4d87-a0c1-d5e82d84090a",
   "metadata": {},
   "source": [
    "## 3) Encoder–Decoder (T5) — summarization (seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3462a0cd-cd76-48d6-b2f1-05690e21df5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941e4d9a62e14f08b8654db27545fd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e8f818fdbc4fdaaa04475d7ae6c8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0807e526d81c4097b7b0e5238b3efa9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860487bddecf48a9a33c799d219c9be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1704707cb4b84980b914fc9515fa30dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7217dbe46a6d4bcba48d28cc60ae69d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers use self-attention to weigh the importance of different tokens. by processing sequences in parallel, they handle long-range dependencies more effectively than recurrent neural networks \n"
     ]
    }
   ],
   "source": [
    "# Goal: Summarize a paragraph with an encoder-decoder model (T5).\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "seq2seq = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "article = (\n",
    "    \"Transformers use self-attention to weigh the importance of different tokens. \"\n",
    "    \"By processing sequences in parallel, they handle long-range dependencies more effectively \"\n",
    "    \"than recurrent neural networks.\"\n",
    ")\n",
    "\n",
    "inputs = tok(\"summarize: \" + article, return_tensors=\"pt\", max_length=256, truncation=True)\n",
    "summary_ids = seq2seq.generate(\n",
    "    **inputs,\n",
    "    max_length=40,\n",
    "    num_beams=4,\n",
    "    length_penalty=1.0,\n",
    "    early_stopping=True\n",
    ")\n",
    "print(tok.decode(summary_ids[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16f1d1-2b82-4750-adc4-c22a63285ac6",
   "metadata": {},
   "source": [
    "## 4) Tiny Self-Attention from scratch (to demystify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69398094-b8d2-437e-9226-a945e96ce242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights (rows sum to 1):\n",
      "tensor([[0.0000, 0.0670, 0.9320, 0.0000],\n",
      "        [0.0020, 0.0410, 0.0680, 0.8890],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 1.0000, 0.0000]])\n",
      "\n",
      "Output shape: torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "# Goal: Show the core of scaled dot-product attention on tiny tensors.\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "B, T, H = 1, 4, 8   # batch=1, tokens=4, hidden=8\n",
    "x = torch.randn(B, T, H)  # toy inputs\n",
    "\n",
    "# Learnable projections (as in a single attention head)\n",
    "Wq = torch.randn(H, H)\n",
    "Wk = torch.randn(H, H)\n",
    "Wv = torch.randn(H, H)\n",
    "\n",
    "Q = x @ Wq        # (B, T, H)\n",
    "K = x @ Wk        # (B, T, H)\n",
    "V = x @ Wv        # (B, T, H)\n",
    "\n",
    "scale = H ** 0.5\n",
    "attn_scores = (Q @ K.transpose(-2, -1)) / scale   # (B, T, T)\n",
    "attn_weights = F.softmax(attn_scores, dim=-1)     # (B, T, T)\n",
    "\n",
    "out = attn_weights @ V                            # (B, T, H)\n",
    "\n",
    "print(\"Attention weights (rows sum to 1):\")\n",
    "print(attn_weights[0].detach().round(decimals=3))\n",
    "print(\"\\nOutput shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e335579-2dd8-40a0-a0c2-5bda2b74c42b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
